{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, pandas, networkx, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import Matcher\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model and reading the document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for multiple files\n",
    "inputtext = \"\"\n",
    "for i in range(1,20):\n",
    "    file_name =f\"{i}.txt\"\n",
    "    #print(file_name)\n",
    "    inputtext+=(open(file_name).read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputtext\n",
    "input_doc = nlp(inputtext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run for single file\n",
    "file_name = '15.txt'\n",
    "input_text = open(file_name).read()\n",
    "#print(type(input_text))\n",
    "input_doc = nlp(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(input_doc.sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing dependencies and entities in text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "options = {\"compact\": True,\"fine_grained\":True, \"ents\":[\"EVENT\",\"PERSON\",\"LOC\",\"ORDINAL\",\"CARDINAL\",\"ORG\"]}\n",
    "displacy.render(sentences, style='dep', jupyter=True,options = options)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(sentences, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting entities from document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEntities(sentence):\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "    prev_dep = \"\"\n",
    "    prev_token = \"\"\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "    \n",
    "    for tok in nlp(sentence):\n",
    "        if not tok.is_punct:\n",
    "            if tok.dep_ == \"compound\":\n",
    "                prefix = tok.text\n",
    "                if prev_dep == \"compound\":\n",
    "                    prefix = prev_token + \" \" + tok.text\n",
    "            if tok.dep_.endswith(\"mod\") == True:\n",
    "                modifier = tok.text\n",
    "                if prev_dep == \"compound\":\n",
    "                    modifier = prev_token + \" \" + tok.text\n",
    "                \n",
    "            #adding subject\n",
    "            if tok.dep_.find(\"subj\") == True:\n",
    "                ent1 = modifier + \" \"+ prefix + \" \"+tok.text\n",
    "                #reset everything\n",
    "                prefix = \"\"\n",
    "                modifier = \"\"\n",
    "                prev_dep = \"\"\n",
    "                prev_token = \"\"\n",
    "            \n",
    "            #adding object\n",
    "            if tok.dep_.find(\"obj\") == True:\n",
    "                ent2 = modifier + \" \" + prefix + \" \"+ tok.text\n",
    "                \n",
    "            #updating variables\n",
    "            prev_dep = tok.dep_\n",
    "            prev_token = tok.text\n",
    "            \n",
    "    return [ent1.strip(), ent2.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getEntities(\"I watched a film\") #checking if it works fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs = []\n",
    "for s in sentences:\n",
    "    entity_pairs.append(getEntities(str(s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelation(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    pattern = [{'DEP':'ROOT'},\n",
    "              {'DEP':'prep','OP':\"?\"},\n",
    "              {'DEP':'agent','OP':\"?\"},  \n",
    "              {'POS':'ADJ','OP':\"?\"}]\n",
    "    \n",
    "    matcher.add(\"matching_1\",None, pattern) #match id\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) -1\n",
    "    span = doc[matches[k][1]:matches[k][2]]\n",
    "    \n",
    "    return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getRelation(\"I watched a film\") #checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations = [getRelation(str(s)) for s in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Counter(relations)) #just to see the most common relations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = [i[0].lower() for i in entity_pairs]\n",
    "target = [i[1].lower() for i in entity_pairs]\n",
    "    \n",
    "s_graph = pandas.DataFrame({'source' : source, 'target' : target, 'edge': relations})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=networkx.from_pandas_edgelist(s_graph, \"source\", \"target\", \n",
    "                          edge_attr=True, create_using=networkx.MultiDiGraph())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,12))\n",
    "\n",
    "pos = networkx.spring_layout(G,k=0.5)\n",
    "networkx.draw(G, with_labels=True, node_color='skyblue', node_size =1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G2 = networkx.DiGraph(G)\n",
    "eigenvector_dict = networkx.eigenvector_centrality(G2,max_iter=1500) # Run eigenvector centrality\n",
    "networkx.set_node_attributes(G, eigenvector_dict, 'eigenvector')\n",
    "sorted_deg = sorted(eigenvector_dict.items(),key=itemgetter(1),reverse=True)\n",
    "for d in sorted_deg:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree_dict = dict(G.degree(G.nodes()))\n",
    "networkx.set_node_attributes(G, degree_dict, 'degree')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_degree = sorted(degree_dict.items(),key=itemgetter(1) ,reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in sorted_degree:\n",
    "    print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
